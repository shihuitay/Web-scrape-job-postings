{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Web Scrape Indeed using Beautiful Soup.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import urllib\n",
        "import requests\n",
        "from bs4 import BeautifulSoup \n",
        "import pandas as pd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnQfOYit2oTi",
        "outputId": "2d4bf779-fd15-4473-8953-71a6a4528875"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.8) or chardet (3.0.4) doesn't match a supported version!\n",
            "  RequestsDependencyWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Web scraping can be divided into a few steps:\n",
        "\n",
        "1. Request the source code/content of a page to a server\n",
        "2. Download the response (usually HTML)\n",
        "3. Parse the downloaded information to identify and extract the information we need"
      ],
      "metadata": {
        "id": "R_WBZo1Q4o4E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uncomment the following to have a look on how the main element looks like:"
      ],
      "metadata": {
        "id": "VeHyBXCh4QCb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# url = 'https://malaysia.indeed.com/jobs?q=data+scientist&l=Kuala+Lumpur' # if I were to search for data scientist roles in Kuala Lumpur\n",
        "# page = requests.get(url) \n",
        "# soup = BeautifulSoup(page.content, 'html.parser')\n",
        "# results = soup.find(id='resultsCol') \n",
        "# print(results.prettify())  # use Prettify so the logged content is easier to read"
      ],
      "metadata": {
        "id": "tlRVBsHf3jEb"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a function so that I can search for any roles at any locations"
      ],
      "metadata": {
        "id": "cURJLHob5JEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_indeed_jobs(job_title, location):\n",
        "  getVars = {'q' : job_title, 'l' : location}\n",
        "  my_page = 0 # the first page\n",
        "\n",
        "  title = []  # create empty lists to store the job details later\n",
        "  company = []\n",
        "  location = []\n",
        "\n",
        "  while my_page<50: # I am going to scrape the data from the first five pages\n",
        "    url = ('https://malaysia.indeed.com/jobs?' + urllib.parse.urlencode(getVars) + f'&start={my_page}')\n",
        "    my_page += 10\n",
        "    page = requests.get(url) # request and download the page\n",
        "    soup = BeautifulSoup(page.content, 'html.parser')  # parse the HTML code using BeautifulSoup\n",
        "    results = soup.find(id='resultsCol') # object that show us all the information inside our main element\n",
        "    indeed_jobs = results.find_all('a', class_=\"tapItem\")\n",
        "    \n",
        "    for indeed_job in indeed_jobs:\n",
        "      job_title = indeed_job.find('span', class_='') # get the job title \n",
        "      job_company = indeed_job.find('span', class_='companyName')  # get the company's name\n",
        "      job_location = indeed_job.find('div', class_='companyLocation') # get the company's location\n",
        "      title.append(job_title.text)\n",
        "      company.append(job_company.text)\n",
        "      location.append(job_location.text)\n",
        "    \n",
        "  job_data = pd.DataFrame({'Title': title, 'Company': company, 'Location': location}) # store the scraped data to a dataframe\n",
        "  job_data.drop_duplicates() # remove any duplicated rows\n",
        "  print(job_data)\n",
        "  job_data.to_csv('jobs.csv', index=False) # save the dataframe to a csv file"
      ],
      "metadata": {
        "id": "MK_shonGQ9M-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_indeed_jobs('data science intern', 'Kuala Lumpur') # if were to search for data science internship opportunities in Kuala Lumpur"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8BDt4ALl2eBz",
        "outputId": "5cb8bce7-7f68-446d-8d8b-e25143c35e51"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                Title  ...                Location\n",
            "0   Intern, Data Analysis & Digitalization, SC Exc...  ...            Kuala Lumpur\n",
            "1   University Internships - Finance & Accounting ...  ...            Kuala Lumpur\n",
            "2      Intern, SQL Data Management (Finance Services)  ...                 Bangsar\n",
            "3                                  Intern - Marketing  ...            Kuala Lumpur\n",
            "4                                              Intern  ...            Kuala Lumpur\n",
            "..                                                ...  ...                     ...\n",
            "61           Product Management Intern (Kuala Lumpur)  ...  Remote in Kuala Lumpur\n",
            "62                                Data Science Intern  ...  Remote in Kuala Lumpur\n",
            "63                          Intern Research Assistant  ...              Mont Kiara\n",
            "64                                      Physio Intern  ...         Mid Valley City\n",
            "65        Software Engineer (Fullstack - Python & JS)  ...            Kuala Lumpur\n",
            "\n",
            "[66 rows x 3 columns]\n"
          ]
        }
      ]
    }
  ]
}